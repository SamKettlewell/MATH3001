{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Handwritten Digits from the MNIST dataset with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example is adapted from Example 2.1 in François Chollet's 'Deep Learning with Python'.[**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing all the relevant modules. For this example we will only require modules from the Keras library: the models module to build the overall network and the layers module to access all the layers we will clip together to build the architecture of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 - Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The popular MNIST dataset contains $70,000$ $28 \\times 28$ greyscale images of handwritten digits from the set $\\{0,1,2,3,4,5,6,7,8,9\\}$. The MNIST dataset is common enough to the machine learning community that it comes pre-loaded with the Keras library[36]. To load this data we simply import it from the ```keras.datasets``` library. Calling ```mnist.load_data()``` returns a tuple of NumPy arrays containing a further two tuples: the first, ```(train images, train labels)```, containing $60,000$ images and their corresponding labels to be used for the process of training the network and the\n",
    "second, ```(test images, test labels)```, containing the remaining $10,000$ images and corresponding labels for testing the performance of the network after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cells below, using NumPy's reshape attribute, we flatten each $28 \\times 28$ array of pixels into a $784 \\times 1$ vector. Each pixel value, $p \\in \\mathbb{Z}$, in this vector is constrained by $0 < p < 256$ where a value of $0$ corresponds to a black pixel, a value of $255$ corresponds to a white pixel and all integer values in-between correspond to shades of grey which increase in brightness as the pixel value p increases. Using this information, we re-scale the vector so that each pixel value, $p \\in (0, 1)$. We do this rescaling to ensure the pixel values are similar in magnitude to the weights [37]. Finally, convert each of the numerical labels to categorical labels with the help of ```to_categorical``` from the ```keras.preprocessing``` package. The data is now in an appropriate form to be fed to the input layer of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 784))\n",
    "test_images = test_images.reshape((10000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Specify the Architecture of the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras' sequential network generating paradigm, we build a neural network with one hidden layer. The architecture of this network is a $784$ unit input layer, followed by a $128$ unit hidden layer and ending in a $10$ unit output layer (one for each digit in $\\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9 \\}$). By specifying the shape of the input data, we indirectly to specify the shape of the input layer. Furthermore, it is only necessary to specify the shape of the input data as Keras can automatically infer the input shape of any subsequent layer from the dimension of the previous one. We use the most suitable ReLu and softmax functions as activation functions for the layers of the network [38]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sam Kettlewell\\Anaconda3\\envs\\TensorFlow_AI_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling ```network.summary()``` displays an overview of the architecture of the network: it shows the number of layers, type of each layer and number of trainable parameters in each layer. For instance, in layer ```dense_9``` we observe there are precisely $100,480 = 784 \\times 128 + 128$ parameters. This is the number we expect since for a dense layer, every neuron in the current layer (128 of them) is connected to every other neuron in the previous layer (784 of them) making exactly $100,352 = 784 \\times 128$ connections/parameters. The additional 128 trainable parameters correspond to the bias. \n",
    "\n",
    "By an exactly similar argument, ```dense_10``` has precisely $1290 = 10 \\times 128 + 10$ trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sam Kettlewell\\Anaconda3\\envs\\TensorFlow_AI_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sam Kettlewell\\Anaconda3\\envs\\TensorFlow_AI_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network.add(layers.Dense(128, activation='relu', input_shape = (784, )))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling ```network.summary()``` displays an overview of the architecture of the network: it shows the number of layers, type of each layer and number of trainable parameters in each layer. For instance, in layer ```dense 9``` we observe there are precisely $100,480 = (784 \\times 128) + 128$ parameters. This is the number we expect since, for a densely connected layer, every neuron in the current layer ($128$ of them) is connected to every neuron in the previous layer ($784$ of them) making exactly $100,352 = 784 \\times 128$ connections, and equivalently, trainable parameters. The additional $128$ trainable parameters correspond to the bias parameters of each neuron in the hidden layer. By an exactly similar argument, ```dense 10``` has precisely $1290 = (10 \\times 128) + 10$ trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Compile the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to select a suitable loss function, optimiser and metric that the network can use to train. Since we are solving a multi-class classification problem, catgeorical-crossentropy is the standard, suitable choice of loss function. We select the rmsprop optimiser to start with, if the network does not give the results we expect, we will return here and change this. Accuracy, the fraction of examples the network correctly classifies, is a suitable metric for this task. (Note, however, it is not suitable for all tasks as we shall see later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sam Kettlewell\\Anaconda3\\envs\\TensorFlow_AI_env\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sam Kettlewell\\Anaconda3\\envs\\TensorFlow_AI_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - Fit the Data to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to train the network. Using the ```network.fit()``` command, we pass the training data ($60,000$ vectorised pixel arrays and their associated labels) to the network.\n",
    "\n",
    "The parameter epochs corresponds to the number of passes of the data through the network. By setting ```epochs = 5``` we are instructing Keras to perform $5$ passes of the data (forward and back) through the network to train the parameters. The parameter batch size corresponds to the number of examples we pass through the network in one pass. An increased batch size is more computationally expensive (requires more memory) but decreases the amount of training timerequired for each epoch.\n",
    "\n",
    "Notice that at the end of each epoch in the output, we are provided with a loss score and an accuracy score. These scores correspond to the value of the loss function and the accuracy score (percentage of training examples correctly classified) at the end of each epoch. After five epochs, we reach an accuracy of approximately $97$ percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sam Kettlewell\\Anaconda3\\envs\\TensorFlow_AI_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Sam Kettlewell\\Anaconda3\\envs\\TensorFlow_AI_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3311 - acc: 0.9089\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1613 - acc: 0.9534\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1149 - acc: 0.9665\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0890 - acc: 0.9743\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0727 - acc: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x235710510f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs = 5, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we call the ```network.evaluate()``` command. This command accepts the $10,000$ testing examples and their associated labels, passes the examples through the network and compares them to the true labels. It calculates the percentage accuracy of the network on the test set to be approximately $97$ percent also. The agreement between these figures is a good indication that the network has not overfit the data, that the network has generalised well and that the network has good predictive power for unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 40us/step\n",
      "Test Accuracy:  0.9745\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print(\"Test Accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**] François Chollet (2017) Deep Learning with Python, : Manning Publications Co. (Example 2.1 available in print and online at <https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/2.1-a-first-look-at-a-neural-network.ipynb>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
